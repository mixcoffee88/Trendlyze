from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium import webdriver

from config import settings
from urllib.parse import urlparse
import json, time, os, platform, re, logging
from utils.result_limiter import ResultLimiter
from utils.common import is_within_days, replace_date, make_result, wait_ready_state
from crawler.crawling_manager import CrawlingManager
from models.elements import InputField, ActionButton

logger = logging.getLogger(__name__)

SITE_NAME = "ZDNET"
URL = "https://zdnet.co.kr/{catCd}"
VIEW_URL = "https://blog.daehong.com/{id}"
TARGET_LIST = [
    # {"catCd": "news/?lstcode=0000", "catNm": "ë‰´ìŠ¤ ìµœì‹ ë‰´ìŠ¤"},
    {"catCd": "news/?lstcode=0010", "catNm": "ë‰´ìŠ¤ ë°©ì†¡/í†µì‹ "},
    {"catCd": "news/?lstcode=0020", "catNm": "ë‰´ìŠ¤ ì»´í“¨íŒ…"},
    {"catCd": "news/?lstcode=0030", "catNm": "ë‰´ìŠ¤ í™ˆ&ëª¨ë°”ì¼"},
    {"catCd": "news/?lstcode=0040", "catNm": "ë‰´ìŠ¤ ì¸í„°ë„·"},
    {"catCd": "news/?lstcode=0050", "catNm": "ë‰´ìŠ¤ ë°˜ë„ì²´/ë””ìŠ¤í”Œë ˆì´"},
    {"catCd": "news/?lstcode=0057", "catNm": "ë‰´ìŠ¤ ì¹´í…Œí¬"},
    {"catCd": "news/?lstcode=0058", "catNm": "ë‰´ìŠ¤ í—¬ìŠ¤ì¼€ì–´"},
    {"catCd": "news/?lstcode=0060", "catNm": "ë‰´ìŠ¤ ê²Œì„"},
    {"catCd": "news/?lstcode=0045", "catNm": "ë‰´ìŠ¤ ì¤‘ê¸°&ìŠ¤íƒ€íŠ¸ì—…"},
    {"catCd": "news/?lstcode=0055", "catNm": "ë‰´ìŠ¤ ìœ í†µ"},
    {"catCd": "news/?lstcode=0073", "catNm": "ë‰´ìŠ¤ ê¸ˆìœµ"},
    {"catCd": "news/?lstcode=0070", "catNm": "ë‰´ìŠ¤ ê³¼í•™"},
    {"catCd": "news/?lstcode=0075", "catNm": "ë‰´ìŠ¤ ë””ì§€í„¸ê²½ì œ"},
    {"catCd": "news/?lstcode=0110", "catNm": "ë‰´ìŠ¤ ì·¨ì—…/HR/êµìœ¡"},
    {"catCd": "news/?lstcode=0100", "catNm": "ë‰´ìŠ¤ ì¸í„°ë·°"},
    {"catCd": "news/?lstcode=0090", "catNm": "ë‰´ìŠ¤ ì¸ì‚¬/ë¶€ìŒ"},
    {"catCd": "news/?lstcode=0120", "catNm": "ë‰´ìŠ¤ ê¸€ë¡œë²Œë‰´ìŠ¤"},
    {"catCd": "special/launch_special_25th.php", "catNm": "ì°½ê°„íŠ¹ì§‘", "pagePass": "Y"},
    {"catCd": "newskey/?lstcode=ì¸ê³µì§€ëŠ¥", "catNm": "ì¸ê³µì§€ëŠ¥"},
    {"catCd": "newskey/?lstcode=ë°°í„°ë¦¬", "catNm": "ë°°í„°ë¦¬"},
    {"catCd": "column/?lstcode=0100", "catNm": "ì¹¼ëŸ¼/ì—°ì¬ ì „ë¬¸ê°€ ì¹¼ëŸ¼"},
    {"catCd": "column/?lstcode=0200", "catNm": "ì¹¼ëŸ¼/ì—°ì¬ ë°ìŠ¤í¬ ì¹¼ëŸ¼"},
    {"catCd": "column/?lstcode=0300", "catNm": "ì¹¼ëŸ¼/ì—°ì¬ ê¸°ì ìˆ˜ì²©"},
    {"catCd": "column/?lstcode=0400", "catNm": "ì¹¼ëŸ¼/ì—°ì¬ ê¸°ì ì—°ì¬"},
    {"catCd": "photo/", "catNm": "í¬í† /ì˜ìƒ"},
]


def crawling(driver: CrawlingManager):

    limiter = ResultLimiter()
    for target in TARGET_LIST:
        logger.debug(f"{target['catNm']} ì§„í–‰ ì‹œì‘")
        is_loop = True
        page_num = 0
        while is_loop:
            page_num += 1
            logger.debug(f"{target['catNm']} / {page_num} ì§„í–‰ ì‹œì‘")
            if target.get("pagePass", "N") == "Y":
                full_url = URL.format(catCd=target["catCd"])
                is_loop = False
            else:
                base_url = target["catCd"]
                page_param = (
                    f"&page={page_num}" if "?" in base_url else f"?page={page_num}"
                )
                full_url = URL.format(catCd=base_url + page_param)
            logger.debug(full_url)
            driver.browser.get(full_url)
            WebDriverWait(driver.browser, 20).until(wait_ready_state())

            # news_box.big ì œê±°
            driver.browser.execute_script(
                """
                const el = document.querySelector('div.news_box.big');
                if (el) el.remove();
            """
            )

            rows = driver.browser.find_elements(
                By.CSS_SELECTOR, "div.news_box > div.newsPost"
            )
            for row in rows:
                try:
                    href = row.find_element(
                        By.CSS_SELECTOR, "div.assetText > a"
                    ).get_attribute("href")
                    title = row.find_element(
                        By.CSS_SELECTOR, "div.assetText > a > h3"
                    ).text.strip()
                    date_span = row.find_element(
                        By.CSS_SELECTOR, "div.assetText > p.byline > span"
                    )
                    date_text = date_span.get_attribute("textContent").strip()
                    if not is_within_days(date_text, day=settings.CRAWLING_LIMIT_DAY):
                        logger.debug(
                            f"â© ë¬´ì‹œ ({settings.CRAWLING_LIMIT_DAY}ì¼ ì´ˆê³¼): {date_text}"
                        )
                        is_loop = False
                        break

                    result_item = make_result(
                        SITE_NAME,
                        target,
                        title,
                        href,
                        replace_date(date_text),
                        driver.getIdx(),
                    )
                    if not limiter.append(result_item):
                        logger.debug(
                            f"ğŸ›‘ ë””ë²„ê·¸ ëª¨ë“œ: {target['catCd']} ìˆ˜ì§‘ ì œí•œ ë„ë‹¬, ì¤‘ë‹¨"
                        )
                        is_loop = False
                        break
                except Exception as e:
                    logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
    driver.closeExtraTabs()
    limiter.results = driver.crawing_content(limiter.results)
    driver.saveResults(SITE_NAME, limiter.results)
