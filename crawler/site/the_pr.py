from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium import webdriver

from config import settings
from urllib.parse import urlparse
import json, time, os, platform, re, logging
from utils.result_limiter import ResultLimiter
from utils.common import is_within_days, replace_date, make_result, wait_ready_state
from crawler.CrawlingManager import CrawlingManager
from models.elements import InputField, ActionButton

logger = logging.getLogger(__name__)

SITE_NAME = "THE PR"
URL = "https://www.the-pr.co.kr/news/articleList.html?page={page_num}"


def crawling(driver: CrawlingManager):

    page_num = 0
    isLoop = True
    parsed = urlparse(URL)
    limiter = ResultLimiter()

    while isLoop:
        page_num += 1
        logger.debug(f"page_num : {page_num}")
        driver.browser.get(URL.format(page_num=page_num))

        # ì²«ë²ˆì¨° ëª©ë¡ ìš”ì†Œ ì²´í¬
        WebDriverWait(driver.browser, 10).until(wait_ready_state())

        # ì²«í˜ì´ì§€ ì¼ ê²½ìš°
        if page_num == 1:
            driver.browser.execute_script(
                f"window.open('{parsed.scheme}://{parsed.netloc}', '_blank');"
            )
            tabs = driver.browser.window_handles

        rows = driver.browser.find_elements(By.CSS_SELECTOR, "#section-list > ul > li")
        for row in rows:
            try:
                start_time = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡

                driver.browser.switch_to.window(tabs[0])  # ì²«ë²ˆì§¸ íƒ­ì—ì„œ íŒŒì‹±
                # logger.debug("âš ï¸ a íƒœê·¸ . row HTML:", row.get_attribute("outerHTML"))

                href = row.find_element(
                    By.CSS_SELECTOR, "div.view-cont > h2.titles > a"
                ).get_attribute("href")
                time.sleep(0.5)  # ë Œë”ëŸ¬ ì¤€ë¹„ ê¸°ë‹¤ë¦¬ê¸°
                driver.browser.switch_to.window(
                    tabs[1]
                )  # ë‘ ë²ˆì§¸ íƒ­ìœ¼ë¡œ ì´ë™í•˜ì—¬ view Urlì—ì„œ dateë° ì œëª© ë‚ ì§œ parsing
                driver.browser.get(href)
                WebDriverWait(driver.browser, 10).until(wait_ready_state())

                date_text = driver.browser.find_element(
                    By.CSS_SELECTOR,
                    "#article-view > div > header > article > div > article:nth-child(1) > ul > li:nth-child(1)",
                ).text.strip()

                if not is_within_days(date_text, day=settings.CRAWLING_LIMIT_DAY):
                    logger.debug(
                        f"â© ë¬´ì‹œ ({settings.CRAWLING_LIMIT_DAY}ì¼ ì´ˆê³¼): {date_text}"
                    )
                    isLoop = False
                    break

                title = driver.browser.find_element(
                    By.CSS_SELECTOR,
                    "#article-view > div > header > article > h1",
                ).text.strip()

                result_item = make_result(
                    SITE_NAME,
                    None,
                    title,
                    href,
                    replace_date(date_text),
                    driver.getIdx(),
                )
                if not limiter.append(result_item):
                    logger.debug(f"ğŸ›‘ ë””ë²„ê·¸ ëª¨ë“œ: ìˆ˜ì§‘ ì œí•œ ë„ë‹¬, ì¤‘ë‹¨")
                    isLoop = False
                    break

                driver.browser.switch_to.window(tabs[0])  # ì²«ë²ˆì§¸ íƒ­ìœ¼ë¡œ ë³€í™˜
                time.sleep(0.5)  # ë Œë”ëŸ¬ ì¤€ë¹„ ê¸°ë‹¤ë¦¬ê¸°
                elapsed_time = time.time() - start_time  # ê²½ê³¼ ì‹œê°„ ê³„ì‚°
                logger.debug(f"â±ï¸ {href} ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            except Exception as e:
                logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
    driver.closeExtraTabs()
    limiter.results = driver.crawing_content(limiter.results)
    driver.saveResults(SITE_NAME, limiter.results)
