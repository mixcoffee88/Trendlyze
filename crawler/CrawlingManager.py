import json, time, os, platform, logging, requests, trafilatura, importlib, uuid, argparse, re, random

from config import settings
from collections import Counter
from urllib.parse import urlparse
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import NoAlertPresentException, TimeoutException

from models.elements import InputField, ActionButton
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
from utils.common import wait_ready_state
from datetime import datetime
from aws.S3Uploader import S3Uploader

# â”€â”€â”€ ë¡œê¹… ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger(__name__)


# -------- ì‚¬ìš©ì ì •ì˜ íƒ€ì„ì•„ì›ƒ ì˜ˆì™¸ --------
class TimeoutError(Exception):
    pass


def handler(signum, frame):
    raise TimeoutError()


# ===== í¬ë¡¤ë§ ë§¤ë‹ˆì € í´ë˜ìŠ¤ =====
class CrawlingManager:
    def __init__(self):
        # í¬ë¡¬ ë¸Œë¼ìš°ì € ì‹¤í–‰ í™˜ê²½(ì˜µì…˜) ì„¤ì • ë° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        logger.debug(f"âœ… CHROME_DRIVER_PATH: {settings.CHROME_DRIVER_PATH}")
        logger.debug(f"âœ… BINARY_PATH: {settings.CHROME_BINARY_PATH}")
        logger.debug(
            f"âœ… os.path.exists(chrome): {os.path.exists(settings.CHROME_BINARY_PATH)}"
        )
        logger.debug(
            f"âœ… os.path.exists(driver): {os.path.exists(settings.CHROME_DRIVER_PATH)}"
        )

        chrome_options = webdriver.ChromeOptions()
        chrome_options.binary_location = str(settings.CHROME_BINARY_PATH)

        # ë””ë²„ê¹…(ë¹„í—¤ë“œë¦¬ìŠ¤/í—¤ë“œë¦¬ìŠ¤) ì„¤ì •
        if not settings.HEADLESS_MODE:
            chrome_options.add_argument("--auto-open-devtools-for-tabs")
        else:
            # print("-")
            chrome_options.add_argument("--headless=new")

        # ë‹¤ì–‘í•œ ìµœì í™” ë° ë³´ì•ˆ, í™˜ê²½ ì˜µì…˜ ì¶”ê°€
        chrome_options.add_argument("--no-sandbox")
        # chrome_options.add_argument("--single-process")
        chrome_options.add_argument("--enable-unsafe-swiftshader")  # âœ… í•µì‹¬
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.7103.113 Safari/537.36"
        )
        chrome_options.add_argument("window-size=1392x1150")
        chrome_options.add_argument("disable-gpu")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--ignore-certificate-errors")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--disable-software-rasterizer")

        chrome_options.add_argument("--disable-background-networking")
        chrome_options.add_argument("--disable-background-timer-throttling")
        chrome_options.add_argument("--disable-client-side-phishing-detection")
        chrome_options.add_argument("--disable-default-apps")
        chrome_options.add_argument("--disable-hang-monitor")
        chrome_options.add_argument("--disable-popup-blocking")
        chrome_options.add_argument("--disable-prompt-on-repost")
        chrome_options.add_argument("--disable-sync")
        chrome_options.add_argument("--metrics-recording-only")

        # chrome_options.add_argument("--disable-images")  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”
        # chrome_options.add_argument("--blink-settings=imagesEnabled=false")
        # chrome_prefs = {"profile.managed_default_content_settings.images": 2}
        # chrome_options.experimental_options["prefs"] = chrome_prefs

        chrome_options.add_argument("--disable-gpu")
        # chrome_options.add_argument("--enable-logging")
        # chrome_options.add_argument("--v=1")
        # chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_experimental_option(
            "excludeSwitches",
            ["enable-logging", "enable-automation", "disable-popup-blocking"],
        )
        chrome_options.add_experimental_option("useAutomationExtension", False)
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")

        # ì•Œë¦¼, íŒì—… ë“± ì°¨ë‹¨ í”„ë¦¬í¼ëŸ°ìŠ¤
        chrome_prefs = {
            "profile.default_content_setting_values.popups": 2,  # íŒì—… ì°¨ë‹¨
            "profile.default_content_setting_values.notifications": 2,  # ì•Œë¦¼ ì°¨ë‹¨
            # "profile.default_content_setting_values.automatic_downloads": 1,  # ë‹¤ìš´ë¡œë“œ í—ˆìš©
        }
        chrome_options.add_experimental_option("prefs", chrome_prefs)

        # OSë³„ í¬ë¡¬ í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        if settings.OS_NAME != "windows":
            chrome_options.add_argument("--user-data-dir=/tmp/chrome-profile")
        else:
            chrome_options.add_argument("--user-data-dir=C:/tmp/tmp/chrome-profile4")

        # caps = DesiredCapabilities.CHROME.copy()
        # caps["pageLoadStrategy"] = "eager"
        # chrome_options.set_capability("pageLoadStrategy", "eager")

        # í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰
        service = Service(str(settings.CHROME_DRIVER_PATH))
        browser = webdriver.Chrome(service=service, options=chrome_options)

        self.browser = browser

        # ìœ ë‹ˆí¬ ì‹ë³„ìš© UUID ìƒì„±
        # self.uuid = str(uuid.uuid4()).replace("-", "")
        # self.uuid = "f9d05846fe4c47c19ff34fe40c1ebe50"
        self.timestamp = datetime.now().strftime("%Y%m%d")
        self.s3 = S3Uploader()
        self.idx = 0  # í¬ë¡¤ë§ ì¸ë±ìŠ¤ ì´ˆê¸°í™”

    def getIdx(self):
        rtnIdx = self.idx
        self.idx += 1
        return rtnIdx

    # ------- ë¡œê·¸ì¸ ë©”ì„œë“œ -------
    def login(
        self, url: str, id: InputField, pwd: InputField, action: ActionButton, wait=None
    ) -> bool:
        """
        ì‚¬ì´íŠ¸ ë¡œê·¸ì¸ ìë™í™”.
        """
        try:
            self.browser.get(url)
            logger.debug(f"âœ… {url} í˜ì´ì§€ ë¡œë“œ ì¤‘...")
            if wait:
                # ì¡°ê±´ì´ ìˆìœ¼ë©´ ëª…ì‹œì ìœ¼ë¡œ ëŒ€ê¸°
                try:
                    WebDriverWait(self.browser, 10).until(wait)
                    logger.debug(f"âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ (wait ì¡°ê±´ ì¶©ì¡±)")
                except Exception as e:
                    logger.error(f"âŒ wait ì¡°ê±´ ì‹¤íŒ¨: {e}")
            else:
                time.sleep(2)

            # ë¡œê·¸ì¸ ì…ë ¥ (ID, PW)
            self.browser.find_element(id.attr, id.attrValue).send_keys(id.value)
            logger.debug(f"âœ… ID ì…ë ¥ ì™„ë£Œ: {id.value}")
            pwd_input = self.browser.find_element(pwd.attr, pwd.attrValue)
            pwd_input.send_keys(pwd.value)
            logger.debug(f"âœ… PWD ì…ë ¥ ì™„ë£Œ: {pwd.value}")

            # ì—”í„° ë˜ëŠ” ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ë¡œê·¸ì¸
            if pwd.afterEnter:
                pwd_input.send_keys(Keys.ENTER)
            else:
                actionEl = self.browser.find_element(action.attr, action.attrValue)
                actionEl.click()

            logger.debug(f"âœ… ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
            # ë¡œê·¸ì¸ í›„ íŠ¹ì • ìš”ì†Œ ë“±ì¥ ëŒ€ê¸°(í•„ìš”ì‹œ)
            if action.wait:
                try:
                    WebDriverWait(self.browser, 10).until(action.wait)
                    logger.debug(f"âœ… ë¡œê·¸ì¸ í›„ ìš”ì†Œ ëŒ€ê¸° ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âš ï¸ ë¡œê·¸ì¸ í›„ ëŒ€ê¸° ì‹¤íŒ¨: {e}")
                    return False

            # ì¶”ê°€ì ì¸ í›„ì²˜ë¦¬ ì½œë°±(ì˜µì…˜)
            if action.waitAfterAction:
                isLoginComplate = action.waitAfterAction(self.browser)
                if not isLoginComplate:
                    logger.error("âŒ ë¡œê·¸ì¸ í™•ì¸ ì‹¤íŒ¨ (í›„ì²˜ë¦¬ ê²°ê³¼ False)")
                    return False
            # time.sleep(2)
            return True
        except Exception as e:
            logger.error(f"[ë¡œê·¸ì¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ] {e}")
            return False

    # ------- ë¶ˆí•„ìš”í•œ íƒ­ ë‹«ê¸° -------
    def closeExtraTabs(self):
        tabs = self.browser.window_handles
        main_tab = tabs[0]

        for tab in tabs:
            if tab != main_tab:
                try:
                    self.browser.switch_to.window(tab)
                    self.browser.close()
                except Exception as e:
                    logger.warning(f"â—íƒ­ ë‹«ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ ì²« ë²ˆì§¸ íƒ­ìœ¼ë¡œ ë³µê·€
        try:
            self.browser.switch_to.window(main_tab)
        except Exception as e:
            logger.error(f"âŒ ì²« ë²ˆì§¸ íƒ­ ë³µê·€ ì‹¤íŒ¨: {e}")

    # ------- ë¸Œë¼ìš°ì € ì¢…ë£Œ -------
    def close(self):
        """
        ë¸Œë¼ìš°ì € ì™„ì „ ì¢…ë£Œ
        """
        self.browser.quit()

    # ------- ìƒì„¸ ë³¸ë¬¸ í¬ë¡¤ë§ -------
    def crawing_content(self, results):
        """
        ê° ê²°ê³¼ì˜ ë§í¬ë³„ë¡œ ë³¸ë¬¸ í¬ë¡¤ë§, ë³¸ë¬¸ ì¶”ì¶œ ë° ì¤‘ë³µì œê±°, íŒì—…/íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
        """
        logger.info("::::::ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘::::::")
        timeout = 20  # í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.browser.set_page_load_timeout(timeout)  # 20ì´ˆ ì´ˆê³¼ ì‹œ

        start_time = time.time()
        origin_counts = Counter(item["metaData"]["origin"] for item in results)
        for origin, count in origin_counts.items():
            logger.debug(f"ğŸ—‚ {origin}: {count}ê±´")

        total = len(results)
        for idx, result in enumerate(results, start=1):
            logger.info(f"ğŸ”— [{idx}/{total}] {result['link']} í¬ë¡¤ë§ ì‹œì‘")
            time.sleep(
                random.uniform(4, 6)
            )  # í¬ë¡¤ë§ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ ì ‘ê·¼ì†ë„ ì œì–´ìœ„í•œ ì¶”ê°€ ì‹œê°„
            link_start = time.time()
            domain = urlparse(result["link"]).netloc

            try:
                logger.info(f"[{idx}/{total}] {result['link']} í¬ë¡¤ë§ ì‹œì‘")
                # ë„ë©”ì¸ ë³„ë¡œ í¬ë¡¤ë§ ë°©ì‹ ë‹¤ë¥´ê²Œ ì²˜ë¦¬(íŠ¹ì • ë„ë©”ì¸ì€ requestsë¡œë§Œ ì²˜ë¦¬)
                try:
                    logger.debug(f"[{idx}] â–¶ driver.get() ì‹œì‘: {result['link']}")

                    if "zdnet.co.kr" in domain:
                        time.sleep(1)
                        try:
                            headers = {
                                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36"
                            }
                            res = requests.get(
                                result["link"], headers=headers, timeout=timeout
                            )
                            res.raise_for_status()
                            logger.debug(
                                f"âœ… requestsë¡œ í˜ì´ì§€ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: {result['link']}"
                            )
                            html = res.text
                        except Exception as e:
                            logger.warning(f"âŒ requests ì‹¤íŒ¨: {result['link']} â†’ {e}")
                            return ""
                    else:
                        # Seleniumì„ í™œìš©í•œ ë¸Œë¼ìš°ì € ì´ë™ ë° ëŒ€ê¸°
                        self.browser.get(result["link"])
                        logger.debug(f"[{idx}] âœ” driver.get() ì™„ë£Œ")

                        logger.debug(f"[{idx}] â–¶ readyState ëŒ€ê¸° ì‹œì‘")
                        try:
                            WebDriverWait(self.browser, 10).until(wait_ready_state())
                            logger.debug(f"[{idx}] âœ” readyState ì™„ë£Œ")

                        except TimeoutException:
                            logger.warning(
                                f"âš ï¸ readyState ë¯¸ì™„ë£Œ â†’ ê°•ì œ ì§„í–‰: {result['link']}"
                            )

                        time.sleep(1)
                        # alert íŒì—… ë‹«ê¸°(ìˆì„ ê²½ìš°)
                        try:
                            alert = self.browser.switch_to.alert
                            alert.dismiss()
                            logger.warning("âš ï¸ íŒì—… alert ë‹«ìŒ")
                        except NoAlertPresentException:
                            pass  # ì •ìƒ

                        logger.debug(f"HTML í˜¸ì¶œ ì™„ë£Œ")
                        time.sleep(2)  # JS ë¡œë”© ëŒ€ê¸° ë³´ê°•
                        html = self.browser.page_source
                        # logger.info(html)
                        logger.info(f"[{idx}/{total}] {result['link']} HTML í˜¸ì¶œ ì™„ë£Œ")

                except TimeoutException:
                    logger.warning(f"âš ï¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {result['link']}")
                    continue

                # parsing_html = self.preprocess_html_for_extraction(domain, html)

                # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ (1ì°¨: trafilatura)
                content = trafilatura.extract(
                    html,
                    with_metadata=False,
                    include_comments=False,
                    include_tables=True,
                    favor_recall=True,
                    output_format="txt",
                    include_links=False,
                )
                # content = self.postprocess_extracted_content(domain, content)
                logger.debug(f'{result["link"]} trafilatura ì „í™˜ ì™„ë£Œ')

                # Fallback: trafilatura ì‹¤íŒ¨ ì‹œ BeautifulSoup
                if not content or len(content) < 300:
                    logger.debug(f'{result["link"]} Fallback : BeautifulSoup ì „í™˜ ì§„í–‰')
                    soup = BeautifulSoup(html, "html.parser")
                    content = soup.get_text(" ", strip=True)
                    logger.debug(f'{result["link"]} Fallback : BeautifulSoup ì „í™˜ ì™„ë£Œ')

                # ë„ˆë¬´ ì§§ì€ ë³¸ë¬¸ì€ None ì²˜ë¦¬
                if len(content) < 200:
                    content = ""

                result["content"] = content or ""  # None ë°©ì§€
                logger.debug(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {result['link']}")
                self.closeExtraTabs()

            except Exception as e:
                result["content"] = ""
                logger.error(
                    f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {result['link']} â†’ {type(e).__name__}: {e}"
                )

            link_elapsed = time.time() - link_start
            logger.info(f"â±ï¸ {result['link']} ì²˜ë¦¬ ì‹œê°„: {link_elapsed:.2f}ì´ˆ")

        empty_count = sum(1 for r in results if not r.get("content"))
        logger.info(f"ğŸ“­ ë³¸ë¬¸ ë¯¸ìˆ˜ì§‘ ê±´ìˆ˜: {empty_count}ê±´ / ì „ì²´ {len(results)}ê±´")

        elapsed_time = time.time() - start_time
        logger.info(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        logger.info("::::::ìƒì„¸ í¬ë¡¤ë§ ì¢…ë£Œ::::::")
        return results

    # ------- site í´ë” ë‚´ ëª¨ë“  crawling() ì‹¤í–‰ -------
    def execCrawlingWebSite(self):
        # logger.info(f"{settings.PROJECT_ROOT}")
        """
        site ë””ë ‰í„°ë¦¬ì˜ ëª¨ë“  .py íŒŒì¼ì—ì„œ crawling(self.browser) í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ ì‹¤í–‰
        """
        site_path = os.path.join(settings.PROJECT_ROOT, "crawler", "site")
        for fname in os.listdir(site_path):
            if fname.endswith(".py"):
                module_name = f"crawler.site.{fname[:-3]}"
                module = importlib.import_module(module_name)
                if hasattr(module, "crawling"):
                    logger.info(f"ì‹¤í–‰: {module_name}.crawling()")
                    try:
                        # if fname[:-3] == "zdnet":
                        #     module.crawling(self)
                        module.crawling(self)
                    except Exception as e:
                        logger.error(f"ì—ëŸ¬: {e}")

    # ------- í¬ë¡¤ë§ ë³¸ë¬¸ ê²°ê³¼ ì €ì¥ -------
    def saveResults(self, module_name, results):
        if not results:
            logger.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        file_full_path = os.path.join(
            settings.CRAWL_DATA_DIR,
            self.timestamp,
            self.uuid,
            f"{module_name.replace(' ', '_')}_result.json",
        )
        self.saveFile(file_full_path, results)

    def saveFile(self, file_full_path, results):
        """
        ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì • íŒŒì¼ ê²½ë¡œì— JSON í¬ë§·ìœ¼ë¡œ ì €ì¥
        """
        try:
            if not results:
                logger.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            self.s3.saveFileToS3(file_full_path, results)
            logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {file_full_path}")
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {file_full_path} â†’ {e}")
            logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {results}")
            raise

    def merge_get_json_files(self, uuid):
        file_full_path = os.path.join(settings.CRAWL_DATA_DIR, self.timestamp, uuid)

        # merged = S3Uploader.loadAllJsonFromPrefix(file_full_path)
        # # í´ë” ë‚´ ëª¨ë“  .json íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        # for fname in os.listdir(file_full_path):
        #     if fname.endswith(".json"):
        #         file_path = os.path.join(file_full_path, fname)
        #         with open(file_path, encoding="utf-8") as f:
        #             data = json.load(f)
        #             if isinstance(data, list):
        #                 merged.extend(data)  # ë¦¬ìŠ¤íŠ¸ë©´ ë³‘í•©
        #             else:
        #                 merged.append(data)  # dict/objectë©´ ê·¸ëƒ¥ append

        try:
            merged = self.s3.loadAllJsonFromPrefix(file_full_path)
            logger.info(f"ğŸ”— merged: {merged}")
            # ì¤‘ë³µ ì œê±°
            # logger.info(f"ğŸ”— ë³‘í•©ëœ ë°ì´í„° ê°œìˆ˜: {len(merged)}ê°œ")
            # merged = self.deduplicate_by_link(merged)
            # logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(merged)}ê°œ í•­ëª©")
            # ìˆœì„œ ë³´ì •
            merged = sorted(merged, key=lambda x: x["idx"])
            self.saveResults("merge", merged)
            return merged
        except Exception as e:
            logger.error(f"âŒ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
            return []

    def preprocess_html_for_extraction(self, domain: str, html: str):
        """
        HTML ë¬¸ìì—´ì—ì„œ .entry-content .contents_style í´ë˜ìŠ¤ë¥¼ ì°¾ì•„ í•´ë‹¹ ë‚´ìš©ì„ ë°˜í™˜í•˜ê³ ,
        ë‚´ë¶€ì˜ footer íƒœê·¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        """
        logger.info(f"in {domain}")
        try:
            soup = BeautifulSoup(html, "html.parser")
            content_html = soup
            if domain == "blog.daehong.com":
                content_html = soup.select_one(".entry-content .contents_style")
                self.remove_tags_by_selector(content_html, ".related-articles")
                self.remove_tags_by_selector(content_html, ".another_category")
                self.remove_tag_and_prev_siblings(
                    content_html, ".entry-content > .contents_style > hr"
                )
            elif domain == "ditoday.com":
                # content_html = soup.select_one(".entry-content")
                self.remove_tag_and_next_siblings(content_html, ".copyright")
                self.remove_tag_with_text(
                    content_html, "ì»¨ìŠˆë¨¸ ëª¨ë¨¼íŠ¸ ë¦¬í¬íŠ¸ ë³´ëŸ¬ ê°€ê¸°", "p"
                )
            elif "brandbrief.co.kr" in domain:
                container = content_html.select_one("#article-view-content-div")
                if container:
                    last_p = container.find_all("p")[-1]
                    last_strong = last_p.find_all("strong")[-1]
                    last_strong.decompose()

            elif domain == "mobiinside.co.kr":
                logger.info("mobiinside.co.kr ì²˜ë¦¬")
                # self.remove_tag_and_next_siblings(content_html, ".post-content > hr")
                self.remove_tag_and_next_siblings(content_html, ".td-post-content > hr")
            # footer íƒœê·¸ ì œê±° (ê³µí†µ ì²˜ë¦¬)
            footers = content_html.find_all("footer")
            for footer in footers:
                footer.decompose()

            # logger.info(f"out 1 {content_html}")
            return str(content_html)
        except Exception as e:
            logger.error(f"âŒ HTML íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # logger.info(f"out 2 {html}")

        return html

    def postprocess_extracted_content(self, domain: str, content: str):
        """ë³¸ë¬¸ ì¶”ì¶œ ì´í›„ ì¶”ê°€ í…ìŠ¤íŠ¸ ê°€ê³µ ë° ì •ì œ(í›„ì²˜ë¦¬)."""
        # íŠ¹ì • íŒ¨í„´ ì œê±° ("ì—°í•©ë‰´ìŠ¤********ì…ë‹ˆë‹¤")
        content = str(content)
        match = re.search(r"ì—°í•©ë‰´ìŠ¤.*?ì…ë‹ˆë‹¤\.", content)
        if match:
            content = content[: match.start()]

        prefix = content[:50]
        match = re.search(r"^\((.{1,10})\)\s{0,5}.{1,10}ê¸°ì\s*=\s*", prefix)
        if match:
            content = content[match.end() :]

        suffix = content[-50:]
        match = re.search(r"\[[^\[\]]{1,20}\s.{1,10}\s(ê¸°ì|íŠ¹íŒŒì›)\]", suffix)
        if match:
            content = content[: -30 + match.start()]

        suffix = content[-50:]  # ë 50ì ì •ë„ë§Œ ê²€ì‚¬
        match = re.search(r"ì œì¼ê¸°íš\s.{2,10}\sí”„ë¡œ\s*\(.*?CDíŒ€\)", suffix)

        if match:
            content = content[: -50 + match.start()]

        # ë§ˆì§€ë§‰ì— ìœ„ì¹˜í•œ ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
        content = re.sub(r"[\w\.-]+@[\w\.-]+\.\w+$", "", content.strip())

        return content

    def remove_tags_by_selector(self, soup, selector):
        """
        ì£¼ì–´ì§„ ì…€ë ‰í„°ë¡œ ì„ íƒëœ ëª¨ë“  íƒœê·¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        """
        remove_tags = soup.select(selector)
        # print(f"[DEBUG] ì„ íƒëœ íƒœê·¸ ìˆ˜: {len(remove_tags)}")
        for remove_tag in remove_tags:
            # logger.info(f"{selector} íƒœê·¸ ì œê±°: {remove_tag.name}")
            remove_tag.decompose()

    def remove_tag_and_prev_siblings(self, soup, selector, is_first=True):
        # <hr> í¬í•¨ ì´ì „ ìš”ì†Œ ì œê±°
        # remove_tag = soup.select_one(selector)
        tags = soup.select(selector)
        if not tags:
            return

        remove_tag = tags[0] if is_first else tags[-1]
        if remove_tag:
            for prev in list(remove_tag.find_previous_siblings()):
                # logger.info(f"ì´ì „ ìš”ì†Œ ì œê±°: {prev.name}")
                if hasattr(prev, "decompose"):
                    prev.decompose()
            remove_tag.decompose()

    def remove_tag_and_next_siblings(self, soup, selector, is_last=True):
        # <hr> í¬í•¨ ì´ì „ ìš”ì†Œ ì œê±°
        tags = soup.select(selector)
        if not tags:
            return
        logger.info(f"ì„ íƒëœ íƒœê·¸ ìˆ˜: {len(tags)}")
        remove_tag = tags[-1] if is_last else tags[0]
        if remove_tag:
            for prev in list(remove_tag.find_next_siblings()):
                logger.info(f"ì´ì „ ìš”ì†Œ ì œê±°: {prev.name}")
                if hasattr(prev, "decompose"):
                    prev.decompose()
            remove_tag.decompose()

    def remove_tag_with_text(self, soup, target_text: str, tag_name: str = "p"):
        """
        íŠ¹ì • í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ìš”ì†Œì˜ ê°€ì¥ ê°€ê¹Œìš´ ìƒìœ„ íƒœê·¸(tag_name)ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

        :param soup: BeautifulSoup íŒŒì‹± ê°ì²´
        :param target_text: ê¸°ì¤€ì´ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¬¸ìì—´
        :param tag_name: ì œê±°í•  ìƒìœ„ íƒœê·¸ ì´ë¦„ (ì˜ˆ: 'p', 'div')
        :return: ìˆ˜ì •ëœ soup ê°ì²´
        """
        target_el = soup.find(string=lambda t: t and target_text in t)

        if target_el:
            container = target_el.find_parent(tag_name)
            if container:
                container.decompose()
                logger.info(f"âœ… '{target_text}' í¬í•¨ <{tag_name}> íƒœê·¸ ì œê±° ì™„ë£Œ")

        return soup
